{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec  \nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.metrics import confusion_matrix,mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nplt.rcParams['font.sans-serif'] = ['Taipei Sans TC Beta']\nimport os\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-17T11:01:39.411568Z","iopub.execute_input":"2021-11-17T11:01:39.41194Z","iopub.status.idle":"2021-11-17T11:01:39.422026Z","shell.execute_reply.started":"2021-11-17T11:01:39.411905Z","shell.execute_reply":"2021-11-17T11:01:39.420852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = pd.read_csv(\"../input/bigdata-hw1/train.csv\")#(17093, 23)\ntest_data = pd.read_csv(\"../input/bigdata-hw1/test.csv\")#(790, 22)\nlocation = dict(N=1, NNE=2, NE=3, ENE=4, E=5, ESE=6, SE=7, SSE=8, S=9, SSW=10, SW=11, WSW=12, W=13, WNW=14, NW=15, NNW=16)\ndf_data = train_data.append( test_data )\ndf_data['Attribute22'] = df_data['Attribute22'].map(dict(Yes=1, No=0))#將Attribute22 string換成num\ndf_data['Attribute23'] = df_data['Attribute23'].map(dict(Yes=1, No=0))#將Attribute23 string換成num\ndf_data['Attribute8'] = df_data['Attribute8'].map(location)#將string方位換成NUM\ndf_data['Attribute10'] = df_data['Attribute10'].map(location)#將string方位換成NUM\ndf_data['Attribute11'] = df_data['Attribute11'].map(location)#將string方位換成NUM","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:39.424755Z","iopub.execute_input":"2021-11-17T11:01:39.425286Z","iopub.status.idle":"2021-11-17T11:01:39.534503Z","shell.execute_reply.started":"2021-11-17T11:01:39.425227Z","shell.execute_reply":"2021-11-17T11:01:39.533289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data['Attribute1'] = df_data['Attribute1'].apply(pd.to_datetime)#將 年月日->月*30+日\ndf_data['year'] = df_data['Attribute1'].dt.year\n#df_data['Attribute1'] = df_data['Attribute1'].dt.month\ndf_data['month'] = df_data['Attribute1'].dt.month\n#df_data['day'] = df_data['Attribute1'].dt.day \n#df_data['Attribute1'] = (df_data['Attribute1'].dt.month-1) * 30 + df_data['Attribute1'].dt.day \ndf_data.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:39.536699Z","iopub.execute_input":"2021-11-17T11:01:39.537023Z","iopub.status.idle":"2021-11-17T11:01:41.191146Z","shell.execute_reply.started":"2021-11-17T11:01:39.536981Z","shell.execute_reply":"2021-11-17T11:01:41.190008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_data['Temp_diff'] = df_data['Attribute20'] - df_data['Attribute21']\ndf_data['wet_diff'] = df_data['Attribute15'] + df_data['Attribute14']\ndf_data['wind_rate'] = df_data['Attribute13'] - df_data['Attribute12']\ndf_data['cloud'] = df_data['Attribute19'] - df_data['Attribute18']\ndf_data['cloud1'] = df_data['Attribute19'] + df_data['Attribute18']\n#df_data['Attribute7_15'] = 0\n#df_data.loc[(df_data[\"Attribute7\"] <= 5) & (df_data[\"Attribute15\"]>= 80), \"Attribute7_15\"] = 1\n#df_data[df_data['Attribute7_15'] == 1]['Attribute23'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:41.192871Z","iopub.execute_input":"2021-11-17T11:01:41.193123Z","iopub.status.idle":"2021-11-17T11:01:41.204685Z","shell.execute_reply.started":"2021-11-17T11:01:41.193082Z","shell.execute_reply":"2021-11-17T11:01:41.203479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''import keras\nfrom keras.models import Sequential \nfrom keras.layers import Dense \n#from keras.optimizers import RMSprop \nfrom keras.callbacks import EarlyStopping \nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import scale\ndp_data = df_data[:len(train_data)]\n#train_data = train_data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\ndp_data_for_7 = dp_data[(dp_data['Attribute3'].notna()) & (dp_data['Attribute4'].notna()) & (dp_data['Attribute5'].notna()) & (dp_data['Attribute7'].notna())\n                              & (dp_data['Attribute8'].notna()) & (dp_data['Attribute9'].notna()) & (dp_data['Attribute10'].notna())\n                               & (dp_data['Attribute11'].notna()) & (dp_data['Attribute12'].notna()) & (dp_data['Attribute13'].notna()) & (dp_data['Attribute14'].notna())\n                               & (dp_data['Attribute15'].notna()) & (dp_data['Attribute16'].notna()) & (dp_data['Attribute17'].notna())\n                               & (dp_data['Attribute20'].notna()) & (dp_data['Attribute21'].notna()) &(dp_data['Attribute22'].notna())]\n\npredict_7 = ['month','year','Attribute2','Attribute3','Attribute4','Attribute5','Attribute8','Attribute9','Attribute10','Attribute11',\n          'Attribute12','Attribute13','Attribute14','Attribute15','Attribute16','Attribute17','Attribute20','Attribute21','Attribute22','Attribute23'] \n\nmodel7 = Sequential() \nmodel7.add(Dense(32, kernel_initializer = 'normal', activation = 'relu',input_shape = (20,))) \nmodel7.add(Dense(64, activation = 'relu')) \nmodel7.add(Dense(32, activation = 'relu'))\nmodel7.add(Dense(1))\nmodel7.compile(\n   loss = 'mse', \n   optimizer = 'Adam',\n   metrics = ['mean_absolute_error']\n)\nhistory = model7.fit(\n   dp_data_for_7[predict_7], dp_data_for_7['Attribute7'],    \n   batch_size=100, \n   epochs = 100, \n   #verbose = 1, \n   validation_split = 0.1, \n   callbacks = [EarlyStopping(monitor = 'val_loss', patience = 20)]\n)'''\ndp_data = df_data[:len(train_data)]\n#train_data = train_data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\ndp_data_for_7 = dp_data[(dp_data['Attribute3'].notna()) & (dp_data['Attribute4'].notna()) & (dp_data['Attribute5'].notna()) & (dp_data['Attribute7'].notna())\n                              & (dp_data['Attribute8'].notna()) & (dp_data['Attribute9'].notna()) & (dp_data['Attribute10'].notna())\n                               & (dp_data['Attribute11'].notna()) & (dp_data['Attribute12'].notna()) & (dp_data['Attribute13'].notna()) & (dp_data['Attribute14'].notna())\n                               & (dp_data['Attribute15'].notna()) & (dp_data['Attribute16'].notna()) & (dp_data['Attribute17'].notna())\n                               & (dp_data['Attribute20'].notna()) & (dp_data['Attribute21'].notna()) &(dp_data['Attribute22'].notna())]\n\npredict_7 = ['month','year','Attribute2','Attribute3','Attribute4','Attribute5','Attribute8','Attribute9','Attribute10','Attribute11',\n          'Attribute12','Attribute13','Attribute14','Attribute15','Attribute16','Attribute17','Attribute20','Attribute21','Attribute22','Attribute23'] \n\nfrom xgboost import XGBRegressor\nmodel7 = XGBRegressor(scale_pos_weight = 3)\n\nmodel7.fit(dp_data_for_7[predict_7], dp_data_for_7['Attribute7'])","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:41.208032Z","iopub.execute_input":"2021-11-17T11:01:41.209402Z","iopub.status.idle":"2021-11-17T11:01:42.32402Z","shell.execute_reply.started":"2021-11-17T11:01:41.209357Z","shell.execute_reply":"2021-11-17T11:01:42.323114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_data))\nprint(len(test_data))\ntrain_data = df_data[:len(train_data)]\ntest_data = df_data[len(train_data):len(train_data) + len(test_data)]\n#train_data = train_data.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\ndf_data = train_data[(train_data['Attribute3'].notna()) & (train_data['Attribute4'].notna()) & (train_data['Attribute5'].notna()) \n                       & (train_data['Attribute8'].notna()) & (train_data['Attribute9'].notna()) & (train_data['Attribute10'].notna())\n                       & (train_data['Attribute11'].notna()) & (train_data['Attribute12'].notna()) & (train_data['Attribute13'].notna()) & (train_data['Attribute14'].notna())\n                       & (train_data['Attribute15'].notna()) & (train_data['Attribute16'].notna()) & (train_data['Attribute17'].notna())\n                       & (train_data['Attribute20'].notna()) & (train_data['Attribute21'].notna()) &(train_data['Attribute22'].notna())]\n\npred7 = model7.predict(df_data[df_data['Attribute7'].isna()][predict_7])\nindex7 = df_data[df_data['Attribute7'].isna()].index\ntrain_data.iloc[index7,6:7] = pred7\ntrain_data = train_data[(train_data['Attribute3'].notna()) & (train_data['Attribute4'].notna()) & (train_data['Attribute5'].notna()) \n                       & (train_data['Attribute8'].notna()) & (train_data['Attribute9'].notna()) & (train_data['Attribute10'].notna())\n                       & (train_data['Attribute11'].notna()) & (train_data['Attribute12'].notna()) & (train_data['Attribute13'].notna()) & (train_data['Attribute14'].notna())\n                       & (train_data['Attribute15'].notna()) & (train_data['Attribute16'].notna()) & (train_data['Attribute17'].notna())\n                       & (train_data['Attribute20'].notna()) & (train_data['Attribute21'].notna()) &(train_data['Attribute22'].notna())]\nprint(len(train_data))\nprint(len(test_data))\n\n#train_data, test_data_for_test = train_test_split(train_data, random_state=666, train_size=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:42.325904Z","iopub.execute_input":"2021-11-17T11:01:42.326544Z","iopub.status.idle":"2021-11-17T11:01:42.389589Z","shell.execute_reply.started":"2021-11-17T11:01:42.326487Z","shell.execute_reply":"2021-11-17T11:01:42.388711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plt.subplot(121)                 # plt.subplot(列數, 行數, 圖形編號)設定第一張圖位置\n#sns.scatterplot(data=train_data, x=\"month\", y=\"Attribute7\", hue=\"Attribute23\")\n#plt.subplot(122)\n#sns.scatterplot(data=train_data, x=\"month\", y=\"Attribute7\", hue=\"Attribute23\")\n#plt.tight_layout()\n#plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:42.392373Z","iopub.execute_input":"2021-11-17T11:01:42.393148Z","iopub.status.idle":"2021-11-17T11:01:42.397326Z","shell.execute_reply.started":"2021-11-17T11:01:42.393078Z","shell.execute_reply":"2021-11-17T11:01:42.3964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data[train_data['Attribute23'] == 1])\nlen(train_data[train_data['Attribute23'] == 0])","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:42.398471Z","iopub.execute_input":"2021-11-17T11:01:42.399757Z","iopub.status.idle":"2021-11-17T11:01:42.420823Z","shell.execute_reply.started":"2021-11-17T11:01:42.399707Z","shell.execute_reply":"2021-11-17T11:01:42.419746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\npredict_final = ['Attribute2','Attribute3','Attribute4','Attribute5','Attribute6','Attribute7','Attribute8','Attribute9','Attribute10','Attribute11',\n          'Attribute12','Attribute13','Attribute14','Attribute15','Attribute16','Attribute17','Attribute20','Attribute21','Attribute22','year','month']\nXGBC = XGBClassifier(scale_pos_weight = 5)\nXGBC.fit(train_data[predict_final],train_data[\"Attribute23\"])\n\npred_XGBC = XGBC.predict(test_data[predict_final])\npred_XGBC = pd.DataFrame(pred_XGBC)\npred_XGBC = pred_XGBC.astype(int)\nid = pd.DataFrame(test_data.index)\nid = id.astype(float)\nsubmission = pd.DataFrame({\n                            \"id\": id[0],\n                            \"ans\": pred_XGBC[0]\n                         })\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:42.422117Z","iopub.execute_input":"2021-11-17T11:01:42.422365Z","iopub.status.idle":"2021-11-17T11:01:44.035261Z","shell.execute_reply.started":"2021-11-17T11:01:42.42233Z","shell.execute_reply":"2021-11-17T11:01:44.034419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import plot_importance\nfrom sklearn.metrics import balanced_accuracy_score\ny = XGBC.predict(test_data_for_test[predict_final])\nplot_importance(XGBC)\nfrom sklearn import metrics\nprint('AUC:',metrics.roc_auc_score(y,test_data_for_test[\"Attribute23\"]))\nprint('ACC:',metrics.accuracy_score(y,test_data_for_test[\"Attribute23\"]))\nprint('Recall:' ,metrics.recall_score(y,test_data_for_test[\"Attribute23\"]))\nprint('F1-score:',metrics.f1_score(y,test_data_for_test[\"Attribute23\"]))\nprint('Precesion:',metrics.precision_score(y,test_data_for_test[\"Attribute23\"]))\nbalanced_accuracy_score(y,test_data_for_test[\"Attribute23\"])","metadata":{"execution":{"iopub.status.busy":"2021-11-17T11:01:44.036717Z","iopub.execute_input":"2021-11-17T11:01:44.037141Z","iopub.status.idle":"2021-11-17T11:01:44.063829Z","shell.execute_reply.started":"2021-11-17T11:01:44.037085Z","shell.execute_reply":"2021-11-17T11:01:44.062837Z"},"trusted":true},"execution_count":null,"outputs":[]}]}